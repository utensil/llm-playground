{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/utensil/llm-playground/blob/main/axolotl_falcon_40b_qlora_deepspeed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97yoSiRvDY7G"
      },
      "source": [
        "# Finetuning falcon-40b\n",
        "\n",
        "- Axolotl+QLoRA\n",
        "- minotaur datasets\n",
        "- deepspeed ZeRO 3 8xGPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCpbQuaxDY7H"
      },
      "source": [
        "<!-- https://jupyterlab.readthedocs.io/en/stable/user/commands.html#commands-list -->\n",
        "<button data-commandLinker-command=\"apputils:change-theme\" data-commandlinker-args='{\"theme\": \"JupyterLab Dark\"}' href=\"#\">Dark theme</button>\n",
        "<button data-commandLinker-command=\"filebrowser:go-to-path\" data-commandlinker-args='{\"path\": \"/workspace/llm-playground/\"}' href=\"#\">llm-playground</button>\n",
        "<button data-commandLinker-command=\"filebrowser:go-to-path\" data-commandlinker-args='{\"path\": \"/workspace/axolotl/\"}' href=\"#\">axolotl</button>\n",
        "<button data-commandLinker-command=\"filebrowser:go-to-path\" data-commandlinker-args='{\"path\": \"/workspace/llm-playground/notebooks/axolotl/runpod\"}' href=\"#\">Runpod notebooks</button>\n",
        "<button data-commandLinker-command=\"filebrowser:go-to-path\" data-commandlinker-args='{\"path\": \"/workspace/axolotl/examples\"}' href=\"#\">axolotl configs</button>\n",
        "<button data-commandLinker-command=\"filebrowser:go-to-path\" data-commandlinker-args='{\"path\": \"/workspace/llm-playground/storage\"}' href=\"#\">Storage</button>\n",
        "<button data-commandLinker-command=\"filebrowser:go-to-path\" data-commandlinker-args='{\"path\": \"/workspace/llm-playground/axolotl-trained\"}' href=\"#\">axolotl-trained</button>\n",
        "<button data-commandLinker-command=\"docmanager:open\" data-commandlinker-args='{\"path\": \"/workspace/axolotl/examples/falcon/config-40b-qlora.yml\"}' href=\"#\">Edit qlora config</button>\n",
        "<button data-commandLinker-command=\"docmanager:open\" data-commandlinker-args='{\"path\": \"/workspace/axolotl/ds_config.json\"}' href=\"#\">Edit ds config</button>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZd-cf70HM2n"
      },
      "source": [
        "## Prepare"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkrtaRkauKLL"
      },
      "source": [
        "### Set HF Cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpIyW4yWuHvd"
      },
      "outputs": [],
      "source": [
        "# %env HF_DATASETS_CACHE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHOfHUYQduRX"
      },
      "outputs": [],
      "source": [
        "#%env TRANSFORMERS_CACHE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oR9oblHTduRY"
      },
      "outputs": [],
      "source": [
        "!rm -rf /root/.cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkaKgWKnduRY"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJMsnPnOduRY"
      },
      "outputs": [],
      "source": [
        "!ln -s /content/cache /root/.cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFqQyPp5HbAm"
      },
      "source": [
        "### HF Login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6wC1lP23B-4",
        "outputId": "9e80e244-b214-4dec-cfba-8d98d0aeba6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token is valid (permission: write).\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "# For axolotl push_dataset_to_hub\n",
        "import os\n",
        "from huggingface_hub import notebook_login, login\n",
        "# Colab:\n",
        "# notebook_login()\n",
        "# RunPod:\n",
        "login(os.environ.get(\"HUGGINGFACE_TOKEN\"), add_to_git_credential=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2AigYR_DY7X"
      },
      "source": [
        "### Update axolotl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQYJdcd7DY7X",
        "outputId": "1244d551-db77-4320-a411-c602d6f28fcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/workspace\n"
          ]
        }
      ],
      "source": [
        "%cd /workspace/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLn5aNACDY7X",
        "outputId": "2165b399-d984-4c59-d6b7-37c03b7e986a",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'axolotl-update'...\n",
            "remote: Enumerating objects: 3139, done.\u001b[K\n",
            "remote: Counting objects: 100% (1333/1333), done.\u001b[K\n",
            "remote: Compressing objects: 100% (344/344), done.\u001b[K\n",
            "remote: Total 3139 (delta 1074), reused 1119 (delta 939), pack-reused 1806\u001b[K\n",
            "Receiving objects: 100% (3139/3139), 1.44 MiB | 6.74 MiB/s, done.\n",
            "Resolving deltas: 100% (1964/1964), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/OpenAccess-AI-Collective/axolotl axolotl-update"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdY5tXtYDY7X"
      },
      "outputs": [],
      "source": [
        "!cp -r axolotl-update/* axolotl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHsKmIUmDY7Y",
        "outputId": "1e69b19b-e820-41e2-876f-130c1b0f7be8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/workspace/axolotl\n"
          ]
        }
      ],
      "source": [
        "%cd /workspace/axolotl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljX3Yir2DY7Y",
        "outputId": "50f1e011-dcd0-4d01-aa87-2203450c439a",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "Changes not staged for commit:\n",
            "  (use \"git add <file>...\" to update what will be committed)\n",
            "  (use \"git restore <file>...\" to discard changes in working directory)\n",
            "\t\u001b[31mmodified:   README.md\u001b[m\n",
            "\t\u001b[31mmodified:   examples/openllama-3b/config.yml\u001b[m\n",
            "\t\u001b[31mmodified:   scripts/runpod-entrypoint.sh\u001b[m\n",
            "\n",
            "Untracked files:\n",
            "  (use \"git add <file>...\" to include in what will be committed)\n",
            "\t\u001b[31mexamples/falcon/ft.yml\u001b[m\n",
            "\t\u001b[31mexamples/falcon/lora.yml\u001b[m\n",
            "\t\u001b[31mexamples/falcon/qlora.yml\u001b[m\n",
            "\t\u001b[31mexamples/huggyllama/\u001b[m\n",
            "\t\u001b[31mexamples/openllama/\u001b[m\n",
            "\n",
            "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"
          ]
        }
      ],
      "source": [
        "!git status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbaorAF2DY7Y",
        "outputId": "01f52eec-42e7-4965-f26d-4364ddb3232c",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obtaining file:///workspace/axolotl\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting transformers@ git+https://github.com/huggingface/transformers.git (from axolotl==0.1)\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-install-i70sd_dc/transformers_4aca752f6b6547c79114332d0942e197\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-install-i70sd_dc/transformers_4aca752f6b6547c79114332d0942e197\n",
            "  Resolved https://github.com/huggingface/transformers.git to commit b89fcccd44508110fd11579a554c1876bc10c0ad\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: bitsandbytes>=0.39.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg (from axolotl==0.1) (0.39.0)\n",
            "Requirement already satisfied: accelerate in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from axolotl==0.1) (0.21.0.dev0)\n",
            "Requirement already satisfied: addict in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from axolotl==0.1) (2.4.0)\n",
            "Requirement already satisfied: fire in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from axolotl==0.1) (0.5.0)\n",
            "Requirement already satisfied: PyYAML==6.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from axolotl==0.1) (6.0)\n",
            "Requirement already satisfied: datasets in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from axolotl==0.1) (2.12.0)\n",
            "Requirement already satisfied: sentencepiece in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from axolotl==0.1) (0.1.99)\n",
            "Requirement already satisfied: wandb in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from axolotl==0.1) (0.15.4)\n",
            "Requirement already satisfied: einops in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from axolotl==0.1) (0.6.1)\n",
            "Requirement already satisfied: xformers in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from axolotl==0.1) (0.0.20)\n",
            "Requirement already satisfied: bert-score==0.3.13 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from axolotl==0.1) (0.3.13)\n",
            "Requirement already satisfied: evaluate==0.4.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from axolotl==0.1) (0.4.0)\n",
            "Requirement already satisfied: rouge-score==0.1.2 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from axolotl==0.1) (0.1.2)\n",
            "Requirement already satisfied: scipy in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from axolotl==0.1) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn==1.2.2 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from axolotl==0.1) (1.2.2)\n",
            "Requirement already satisfied: torch>=1.0.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from bert-score==0.3.13->axolotl==0.1) (2.0.1)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from bert-score==0.3.13->axolotl==0.1) (2.0.2)\n",
            "Requirement already satisfied: numpy in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from bert-score==0.3.13->axolotl==0.1) (1.24.3)\n",
            "Requirement already satisfied: requests in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from bert-score==0.3.13->axolotl==0.1) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from bert-score==0.3.13->axolotl==0.1) (4.65.0)\n",
            "Requirement already satisfied: matplotlib in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from bert-score==0.3.13->axolotl==0.1) (3.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from bert-score==0.3.13->axolotl==0.1) (23.1)\n",
            "Requirement already satisfied: dill in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from evaluate==0.4.0->axolotl==0.1) (0.3.6)\n",
            "Requirement already satisfied: xxhash in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from evaluate==0.4.0->axolotl==0.1) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from evaluate==0.4.0->axolotl==0.1) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from evaluate==0.4.0->axolotl==0.1) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from evaluate==0.4.0->axolotl==0.1) (0.15.1)\n",
            "Requirement already satisfied: responses<0.19 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from evaluate==0.4.0->axolotl==0.1) (0.18.0)\n",
            "Requirement already satisfied: absl-py in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from rouge-score==0.1.2->axolotl==0.1) (1.4.0)\n",
            "Requirement already satisfied: nltk in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from rouge-score==0.1.2->axolotl==0.1) (3.8.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from rouge-score==0.1.2->axolotl==0.1) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from scikit-learn==1.2.2->axolotl==0.1) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from scikit-learn==1.2.2->axolotl==0.1) (3.1.0)\n",
            "Requirement already satisfied: psutil in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from accelerate->axolotl==0.1) (5.9.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from datasets->axolotl==0.1) (12.0.0)\n",
            "Requirement already satisfied: aiohttp in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from datasets->axolotl==0.1) (3.8.4)\n",
            "Requirement already satisfied: filelock in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from transformers@ git+https://github.com/huggingface/transformers.git->axolotl==0.1) (3.12.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from transformers@ git+https://github.com/huggingface/transformers.git->axolotl==0.1) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from transformers@ git+https://github.com/huggingface/transformers.git->axolotl==0.1) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from transformers@ git+https://github.com/huggingface/transformers.git->axolotl==0.1) (0.3.1)\n",
            "Requirement already satisfied: termcolor in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from fire->axolotl==0.1) (2.3.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from wandb->axolotl==0.1) (8.1.3)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from wandb->axolotl==0.1) (3.1.31)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from wandb->axolotl==0.1) (1.25.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from wandb->axolotl==0.1) (0.4.0)\n",
            "Requirement already satisfied: pathtools in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from wandb->axolotl==0.1) (0.1.2)\n",
            "Requirement already satisfied: setproctitle in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from wandb->axolotl==0.1) (1.3.2)\n",
            "Requirement already satisfied: setuptools in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from wandb->axolotl==0.1) (67.8.0)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from wandb->axolotl==0.1) (1.4.4)\n",
            "Requirement already satisfied: typing-extensions in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from wandb->axolotl==0.1) (4.6.3)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from wandb->axolotl==0.1) (4.23.2)\n",
            "Requirement already satisfied: pyre-extensions==0.0.29 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from xformers->axolotl==0.1) (0.0.29)\n",
            "Requirement already satisfied: typing-inspect in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from pyre-extensions==0.0.29->xformers->axolotl==0.1) (0.9.0)\n",
            "Requirement already satisfied: sympy in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (1.12)\n",
            "Requirement already satisfied: networkx in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (3.1)\n",
            "Requirement already satisfied: jinja2 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (3.1.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (2.0.0)\n",
            "Requirement already satisfied: wheel in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (0.40.0)\n",
            "Requirement already satisfied: cmake in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (3.26.4)\n",
            "Requirement already satisfied: lit in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (16.0.5.post0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from aiohttp->datasets->axolotl==0.1) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from aiohttp->datasets->axolotl==0.1) (3.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from aiohttp->datasets->axolotl==0.1) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from aiohttp->datasets->axolotl==0.1) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from aiohttp->datasets->axolotl==0.1) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from aiohttp->datasets->axolotl==0.1) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from aiohttp->datasets->axolotl==0.1) (1.3.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb->axolotl==0.1) (4.0.10)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from pandas>=1.0.1->bert-score==0.3.13->axolotl==0.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from pandas>=1.0.1->bert-score==0.3.13->axolotl==0.1) (2023.3)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from pandas>=1.0.1->bert-score==0.3.13->axolotl==0.1) (2023.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from requests->bert-score==0.3.13->axolotl==0.1) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from requests->bert-score==0.3.13->axolotl==0.1) (2.0.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from requests->bert-score==0.3.13->axolotl==0.1) (2023.5.7)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from matplotlib->bert-score==0.3.13->axolotl==0.1) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from matplotlib->bert-score==0.3.13->axolotl==0.1) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from matplotlib->bert-score==0.3.13->axolotl==0.1) (4.40.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from matplotlib->bert-score==0.3.13->axolotl==0.1) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from matplotlib->bert-score==0.3.13->axolotl==0.1) (9.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from matplotlib->bert-score==0.3.13->axolotl==0.1) (3.0.9)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from matplotlib->bert-score==0.3.13->axolotl==0.1) (5.12.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->axolotl==0.1) (5.0.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib->bert-score==0.3.13->axolotl==0.1) (3.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from jinja2->torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from sympy->torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (1.3.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from typing-inspect->pyre-extensions==0.0.29->xformers->axolotl==0.1) (1.0.0)\n",
            "Installing collected packages: axolotl\n",
            "  Attempting uninstall: axolotl\n",
            "    Found existing installation: axolotl 0.1\n",
            "    Uninstalling axolotl-0.1:\n",
            "      Successfully uninstalled axolotl-0.1\n",
            "  Running setup.py develop for axolotl\n",
            "Successfully installed axolotl-0.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Adru8lVlzhLi",
        "outputId": "084d6af3-b595-4279-d599-0d49a4460b3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting ds_accelerator to cuda (auto detect)\n",
            "--------------------------------------------------\n",
            "DeepSpeed C++/CUDA extension op report\n",
            "--------------------------------------------------\n",
            "NOTE: Ops not installed will be just-in-time (JIT) compiled at\n",
            "      runtime if needed. Op compatibility means that your system\n",
            "      meet the required dependencies to JIT install the op.\n",
            "--------------------------------------------------\n",
            "JIT compiled ops requires ninja\n",
            "ninja .................. \u001b[92m[OKAY]\u001b[0m\n",
            "--------------------------------------------------\n",
            "op name ................ installed .. compatible\n",
            "--------------------------------------------------\n",
            "async_io ............... \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "cpu_adagrad ............ \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "cpu_adam ............... \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "fused_adam ............. \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "fused_lamb ............. \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "quantizer .............. \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "random_ltd ............. \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0\n",
            "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible\n",
            "sparse_attn ............ \u001b[93m[NO]\u001b[0m ....... \u001b[93m[NO]\u001b[0m\n",
            "spatial_inference ...... \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "transformer ............ \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "stochastic_transformer . \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "transformer_inference .. \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "utils .................. \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "--------------------------------------------------\n",
            "DeepSpeed general environment info:\n",
            "torch install path ............... ['/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch']\n",
            "torch version .................... 2.0.1+cu117\n",
            "deepspeed install path ........... ['/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed']\n",
            "deepspeed info ................... 0.9.3+52907a66, 52907a66, master\n",
            "torch cuda version ............... 11.7\n",
            "torch hip version ................ None\n",
            "nvcc version ..................... 11.8\n",
            "deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8\n"
          ]
        }
      ],
      "source": [
        "!ds_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_9eIEonzhLi",
        "outputId": "fc687682-4a63-4400-fb40-e769dd00bd29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch                    2.0.1\n",
            "torchaudio               2.0.1+cu118\n",
            "torchvision              0.15.1+cu118\n"
          ]
        }
      ],
      "source": [
        "!pip list|grep torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SD1Ahx1QG5xm"
      },
      "source": [
        "### Init Storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBBF-lS2HVPG",
        "outputId": "400163fc-7e26-4927-d66a-900e7cc9d060"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/axolotl-trained is already a clone of https://huggingface.co/utensil/axolotl-trained. Make sure you pull the latest changes with `repo.git_pull()`.\n"
          ]
        }
      ],
      "source": [
        "!python /workspace/llm-playground/helper/storage.py utensil/axolotl-trained /content/ -m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_FP2VeXH0Un",
        "outputId": "3402413c-9d4e-4d69-8c8d-6f9ded6b16c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "falcon-qlora-40b-gsm8k\n"
          ]
        }
      ],
      "source": [
        "!ls /content/axolotl-trained"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raYU6HmYduRa"
      },
      "source": [
        "### Reinstall PyTorch with CUDA 11.8 (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ym6v1nl6duRa"
      },
      "outputs": [],
      "source": [
        "!pip3 install -U torch --index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0A0aPWJzhLk"
      },
      "outputs": [],
      "source": [
        "!pip list|grep torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sPSrhKPHIrS"
      },
      "source": [
        "### Reinstall deepspeed (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_PIACM6duRa"
      },
      "outputs": [],
      "source": [
        "!ds_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1-u6vhWHvt6"
      },
      "outputs": [],
      "source": [
        "# !pip uninstall deepspeed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIpqugJOHuhh"
      },
      "outputs": [],
      "source": [
        "# !TORCH_CUDA_ARCH_LIST=\"3.5;5.0;6.0;6.1;7.0;7.5;8.0;8.6+PTX\" DS_BUILD_OPS=1 pip install deepspeed --global-option=\"build_ext\" --global-option=\"-j8\" # --global-option=\"bdist_wheel\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_tw4OcVHQdK"
      },
      "source": [
        "### Init Configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJvyUmZdktu0",
        "outputId": "25275dd8-4787-4b77-f7bf-4f2bbe66f0b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/workspace/axolotl\n"
          ]
        }
      ],
      "source": [
        "%cd /workspace/axolotl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mC48y25Lkqa5"
      },
      "outputs": [],
      "source": [
        "# Try no config\n",
        "# !accelerate config default"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cL5E8urQEXiL",
        "outputId": "1a7ac7ab-c81d-4e10-ac9e-df10ed3181c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ds_config.json\n"
          ]
        }
      ],
      "source": [
        "%%writefile ds_config.json\n",
        "{\n",
        "    \"zero_optimization\": {\n",
        "        \"stage\": 3,\n",
        "        \"offload_optimizer\": {\n",
        "            \"device\": \"cpu\",\n",
        "            \"pin_memory\": true\n",
        "        },\n",
        "        \"offload_param\": {\n",
        "            \"device\": \"cpu\",\n",
        "            \"pin_memory\": true\n",
        "        },\n",
        "        \"overlap_comm\": true,\n",
        "        \"contiguous_gradients\": true,\n",
        "        \"sub_group_size\": 0,\n",
        "        \"reduce_bucket_size\": \"auto\",\n",
        "        \"stage3_prefetch_bucket_size\": \"auto\",\n",
        "        \"stage3_param_persistence_threshold\": \"auto\",\n",
        "        \"stage3_max_live_parameters\": 0,\n",
        "        \"stage3_max_reuse_distance\": 0,\n",
        "        \"stage3_gather_16bit_weights_on_model_save\": true\n",
        "    },\n",
        "   \"bf16\": {\n",
        "        \"enabled\": \"auto\"\n",
        "    },\n",
        "    \"fp16\": {\n",
        "        \"enabled\": \"auto\",\n",
        "        \"auto_cast\": false,\n",
        "        \"loss_scale\": 0,\n",
        "        \"initial_scale_power\": 32,\n",
        "        \"loss_scale_window\": 1000,\n",
        "        \"hysteresis\": 2,\n",
        "        \"min_loss_scale\": 1\n",
        "    },\n",
        "    \"optimizer\": {\n",
        "        \"type\": \"AdamW\",\n",
        "        \"params\": {\n",
        "          \"lr\": \"auto\",\n",
        "          \"betas\": [\n",
        "            0.9,\n",
        "            0.999\n",
        "          ],\n",
        "          \"eps\": 1e-8,\n",
        "          \"weight_decay\": \"auto\"\n",
        "        }\n",
        "    },\n",
        "    \"scheduler\": {\n",
        "        \"type\": \"OneCycle\",\n",
        "        \"params\": {\n",
        "          \"cycle_min_lr\": 0.00001,\n",
        "          \"cycle_max_lr\": 0.00003,\n",
        "          \"cycle_first_step_size\": 120\n",
        "        }\n",
        "      },\n",
        "    \"train_batch_size\": \"auto\",\n",
        "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
        "    \"wall_clock_breakdown\": false\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efZJw_gCDY7J",
        "outputId": "30a5cd1b-eda1-4384-f9d5-090dd376961d",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing examples/falcon/config-40b-qlora.yml\n"
          ]
        }
      ],
      "source": [
        "%%writefile examples/falcon/config-40b-qlora.yml\n",
        "# 1b: tiiuae/falcon-rw-1b\n",
        "# 7b: tiiuae/falcon-7b\n",
        "# 40b: tiiuae/falcon-40b\n",
        "base_model: tiiuae/falcon-40b\n",
        "base_model_config: tiiuae/falcon-40b\n",
        "# required by falcon custom model code: https://huggingface.co/tiiuae/falcon-7b/tree/main\n",
        "trust_remote_code: true\n",
        "model_type: AutoModelForCausalLM\n",
        "tokenizer_type: AutoTokenizer\n",
        "load_in_8bit: false\n",
        "# enable 4bit for QLoRA\n",
        "load_in_4bit: true\n",
        "gptq: false\n",
        "strict: false\n",
        "\n",
        "push_dataset_to_hub: utensil\n",
        "hf_use_auth_token: true\n",
        "\n",
        "datasets:\n",
        "  - path: QingyiSi/Alpaca-CoT\n",
        "    data_files:\n",
        "      - Chain-of-Thought/formatted_cot_data/gsm8k_train.json\n",
        "    type: \"alpaca:chat\"\n",
        "\n",
        "dataset_prepared_path: last_run_prepared\n",
        "val_set_size: 0.01\n",
        "# enable QLoRA\n",
        "adapter: qlora\n",
        "lora_model_dir:\n",
        "sequence_len: 2048\n",
        "max_packed_sequence_len:\n",
        "\n",
        "# hyperparameters from QLoRA paper Appendix B.2\n",
        "# \"We find hyperparameters to be largely robust across datasets\"\n",
        "lora_r: 64\n",
        "lora_alpha: 16\n",
        "# 0.1 for models up to 13B\n",
        "# 0.05 for 33B and 65B models\n",
        "lora_dropout: 0.05\n",
        "# add LoRA modules on all linear layers of the base model\n",
        "lora_target_modules:\n",
        "lora_target_linear: true\n",
        "lora_fan_in_fan_out:\n",
        "\n",
        "wandb_project: falcon-qlora\n",
        "wandb_watch:\n",
        "wandb_run_id:\n",
        "wandb_log_model:\n",
        "output_dir: /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
        "\n",
        "# QLoRA paper Table 9\n",
        "# - 16 for 7b & 13b\n",
        "# - 32 for 33b, 64 for 64b\n",
        "# Max size tested on A6000\n",
        "# - 7b: 40\n",
        "# - 40b: 4\n",
        "# decrease if OOM, increase for max VRAM utilization\n",
        "micro_batch_size: 1\n",
        "gradient_accumulation_steps: 1\n",
        "num_epochs: 3\n",
        "# Optimizer for QLoRA\n",
        "# optimizer: paged_adamw_32bit\n",
        "torchdistx_path:\n",
        "# lr_scheduler: cosine\n",
        "# QLoRA paper Table 9\n",
        "# - 2e-4 for 7b & 13b\n",
        "# - 1e-4 for 33b & 64b\n",
        "learning_rate: 0.0002\n",
        "train_on_inputs: false\n",
        "group_by_length: false\n",
        "bf16: true\n",
        "fp16: false\n",
        "tf32: true\n",
        "gradient_checkpointing: true\n",
        "# stop training after this many evaluation losses have increased in a row\n",
        "# https://huggingface.co/transformers/v4.2.2/_modules/transformers/trainer_callback.html#EarlyStoppingCallback\n",
        "early_stopping_patience: 3\n",
        "resume_from_checkpoint:\n",
        "auto_resume_from_checkpoints: true\n",
        "local_rank:\n",
        "logging_steps: 1\n",
        "xformers_attention: true\n",
        "flash_attention:\n",
        "gptq_groupsize:\n",
        "gptq_model_v1:\n",
        "warmup_steps: 10\n",
        "eval_steps: 5\n",
        "save_steps: 10\n",
        "debug:\n",
        "deepspeed:\n",
        "weight_decay: 0.01\n",
        "fsdp:\n",
        "fsdp_config:\n",
        "special_tokens:\n",
        "  pad_token: \"<|endoftext|>\"\n",
        "  bos_token: \">>ABSTRACT<<\"\n",
        "  eos_token: \"<|endoftext|>\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iz9sbxRAElkU"
      },
      "outputs": [],
      "source": [
        "%%writefile scripts/ft.py\n",
        "import os\n",
        "from pathlib import Path\n",
        "import fire\n",
        "import logging\n",
        "import finetune\n",
        "from axolotl.utils.trainer import setup_trainer as setup_trainer_orig\n",
        "\n",
        "logging.basicConfig(level=os.getenv(\"LOG_LEVEL\", \"INFO\"))\n",
        "\n",
        "def train_ex(\n",
        "    config: Path = Path(\"configs/\"),\n",
        "    prepare_ds_only: bool = False,\n",
        "    **kwargs,\n",
        "):\n",
        "  logging.info('train_ex before')\n",
        "  finetune.train(config, prepare_ds_only, **kwargs)\n",
        "  logging.info('train_ex after')\n",
        "\n",
        "def setup_trainer_ex(cfg, train_dataset, eval_dataset, model, tokenizer):\n",
        "  logging.info('setup_trainer_ex before')\n",
        "  logging.info(f'cfg.some_config = {cfg.some_config}')\n",
        "  trainer = setup_trainer_orig(cfg, train_dataset, eval_dataset, model, tokenizer)\n",
        "  logging.info('setup_trainer_ex after')\n",
        "  return trainer\n",
        "\n",
        "finetune.setup_trainer = setup_trainer_ex\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    fire.Fire(train_ex)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTWOnrpzEr-1",
        "outputId": "aae51ccf-bb6b-4f5d-d133-866e0aae9e5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: ACCELERATE_USE_DEEPSPEED=true\n"
          ]
        }
      ],
      "source": [
        "%env ACCELERATE_USE_DEEPSPEED=true"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vj6CD_zZHUpG"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CqrhYq-DY7J"
      },
      "source": [
        "## #1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m66yoSyzK7uC",
        "outputId": "5d3c57e0-0979-432d-ebe3-ab0c952a78ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/workspace/axolotl\n"
          ]
        }
      ],
      "source": [
        "%cd /workspace/axolotl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "joQh6mGBm3_J",
        "outputId": "9eb84312-03a6-424f-fdc4-cde15277d680"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# 1b: tiiuae/falcon-rw-1b\n",
            "# 7b: tiiuae/falcon-7b\n",
            "# 40b: tiiuae/falcon-40b\n",
            "base_model: tiiuae/falcon-40b\n",
            "base_model_config: tiiuae/falcon-40b\n",
            "# required by falcon custom model code: https://huggingface.co/tiiuae/falcon-7b/tree/main\n",
            "trust_remote_code: true\n",
            "model_type: AutoModelForCausalLM\n",
            "tokenizer_type: AutoTokenizer\n",
            "load_in_8bit: false\n",
            "# enable 4bit for QLoRA\n",
            "load_in_4bit: true\n",
            "gptq: false\n",
            "strict: false\n",
            "\n",
            "push_dataset_to_hub: utensil\n",
            "hf_use_auth_token: true\n",
            "\n",
            "datasets:\n",
            "  - path: QingyiSi/Alpaca-CoT\n",
            "    data_files:\n",
            "      - Chain-of-Thought/formatted_cot_data/gsm8k_train.json\n",
            "    type: \"alpaca:chat\"\n",
            "\n",
            "dataset_prepared_path: last_run_prepared\n",
            "val_set_size: 0.01\n",
            "# enable QLoRA\n",
            "adapter: qlora\n",
            "lora_model_dir:\n",
            "sequence_len: 2048\n",
            "max_packed_sequence_len:\n",
            "\n",
            "# hyperparameters from QLoRA paper Appendix B.2\n",
            "# \"We find hyperparameters to be largely robust across datasets\"\n",
            "lora_r: 64\n",
            "lora_alpha: 16\n",
            "# 0.1 for models up to 13B\n",
            "# 0.05 for 33B and 65B models\n",
            "lora_dropout: 0.05\n",
            "# add LoRA modules on all linear layers of the base model\n",
            "lora_target_modules:\n",
            "lora_target_linear: true\n",
            "lora_fan_in_fan_out:\n",
            "\n",
            "wandb_project: falcon-qlora\n",
            "wandb_watch:\n",
            "wandb_run_id:\n",
            "wandb_log_model:\n",
            "output_dir: /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
            "\n",
            "# QLoRA paper Table 9\n",
            "# - 16 for 7b & 13b\n",
            "# - 32 for 33b, 64 for 64b\n",
            "# Max size tested on A6000\n",
            "# - 7b: 40\n",
            "# - 40b: 4\n",
            "# decrease if OOM, increase for max VRAM utilization\n",
            "micro_batch_size: 1\n",
            "gradient_accumulation_steps: 1\n",
            "num_epochs: 3\n",
            "# Optimizer for QLoRA\n",
            "# optimizer: paged_adamw_32bit\n",
            "torchdistx_path:\n",
            "# lr_scheduler: cosine\n",
            "# QLoRA paper Table 9\n",
            "# - 2e-4 for 7b & 13b\n",
            "# - 1e-4 for 33b & 64b\n",
            "learning_rate: 0.0002\n",
            "train_on_inputs: false\n",
            "group_by_length: false\n",
            "bf16: true\n",
            "fp16: false\n",
            "tf32: true\n",
            "gradient_checkpointing: true\n",
            "# stop training after this many evaluation losses have increased in a row\n",
            "# https://huggingface.co/transformers/v4.2.2/_modules/transformers/trainer_callback.html#EarlyStoppingCallback\n",
            "early_stopping_patience: 3\n",
            "resume_from_checkpoint:\n",
            "auto_resume_from_checkpoints: true\n",
            "local_rank:\n",
            "logging_steps: 1\n",
            "xformers_attention: true\n",
            "flash_attention:\n",
            "gptq_groupsize:\n",
            "gptq_model_v1:\n",
            "warmup_steps: 10\n",
            "eval_steps: 5\n",
            "save_steps: 10\n",
            "debug:\n",
            "deepspeed:\n",
            "weight_decay: 0.01\n",
            "fsdp:\n",
            "fsdp_config:\n",
            "special_tokens:\n",
            "  pad_token: \"<|endoftext|>\"\n",
            "  bos_token: \">>ABSTRACT<<\"\n",
            "  eos_token: \"<|endoftext|>\"\n"
          ]
        }
      ],
      "source": [
        "!cat examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULwPlElbm73f"
      },
      "outputs": [],
      "source": [
        "#%%writefile examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jICMPJuomFsx",
        "outputId": "241a2589-fb00-414b-b27c-d5fc27e131ae",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting ds_accelerator to cuda (auto detect)\n",
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `1`\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDvnE4umHheXhWsDJbbukYvvyc47/mC4z8syS93btA72T90WDrQagOy5O+DrhdXOvr5i/JwsTlAImy57eLRrtRFOrQq73jyi7Dzo0tvrAiNLVgX2q2dFLoplRyXDXiVYLPmPieMWQOeUCLeSb8FC5zzllcocZwjMXpxScDerZqnlAR0ccpSkGyKIod4ZMkn/29A/C5kHEb/wT8cOAq+MWJ/2okZZgbiR0AMV4DynAkrtcx9JnJnTs9chiMyH+dyCS42Ai24sHWJBkQo6TfxXkyKo9GOpu3Y2WLgrHyaot9Lk5mA1mujyIWdlReD2nvjeCQKjl3KW3xZ73m4nD97MydWSWoJfEWlr+VZvk8EWsZk3CYLZCIBLdod6xXJJ0DD0pvTIq11c8VB7XkgVjapuU/sC8M6HFzHW/NBeE+xX/txPkZkIGqrnxeQ0AtBXdN9ukyNGhGzTkPYJNliiYpY0dCvVuz/BJ2FawFTQGnD1EHOenUCRajREFGCbKoYZqi40j8= utensil@Utensils-MacBook-Pro.local')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "Setting ds_accelerator to cuda (auto detect)\n",
            "WARNING:root:`trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\n",
            "INFO:root:loading tokenizer... tiiuae/falcon-40b\n",
            "Using bos_token, but it is not set yet.\n",
            "Using pad_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/utensil___parquet/utensil--31a4e867d804a957707db033c9abcd13-7b1208a23cdad6e9/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
            "100%|| 1/1 [00:00<00:00, 540.78it/s]\n",
            "INFO:root:loading model and peft_config...\n",
            "Loading checkpoint shards: 100%|| 9/9 [01:21<00:00,  9.07s/it]\n",
            "INFO:root:converting PEFT model w/ prepare_model_for_kbit_training\n",
            "INFO:root:found linear modules: ['query_key_value', 'dense_h_to_4h', 'dense_4h_to_h', 'dense']\n",
            "trainable params: 444,334,080 || all params: 21,363,310,592 || trainable%: 2.0798933671188187\n",
            "INFO:root:Compiling torch model\n",
            "INFO:root:Pre-saving adapter config to /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
            "INFO:root:Starting trainer...\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mutensil\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/workspace/axolotl/wandb/run-20230614_075239-d8he4ei0\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33methereal-blaze-27\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/utensil/falcon-qlora\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/utensil/falcon-qlora/runs/d8he4ei0\u001b[0m\n",
            "  0%|                                                 | 0/22194 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "{'loss': 0.7804, 'learning_rate': 2e-05, 'epoch': 0.0}                          \n",
            "{'loss': 0.8057, 'learning_rate': 4e-05, 'epoch': 0.0}                          \n",
            "{'loss': 1.348, 'learning_rate': 6e-05, 'epoch': 0.0}                           \n",
            "{'loss': 1.0281, 'learning_rate': 8e-05, 'epoch': 0.0}                          \n",
            "{'loss': 0.6128, 'learning_rate': 0.0001, 'epoch': 0.0}                         \n",
            "  0%|                                      | 5/22194 [00:09<10:34:20,  1.72s/it]Traceback (most recent call last):\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 329, in <module>\n",
            "    fire.Fire(train)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 141, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 475, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 316, in train\n",
            "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1537, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1893, in _inner_training_loop\n",
            "    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 2194, in _maybe_log_save_evaluate\n",
            "    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 2926, in evaluate\n",
            "    output = eval_loop(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 3041, in evaluation_loop\n",
            "    _, _ = deepspeed_init(self, num_training_steps=0, inference=True)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/deepspeed.py\", line 351, in deepspeed_init\n",
            "    raise ValueError(\"ZeRO inference only makes sense with ZeRO Stage 3 - please adjust your config\")\n",
            "ValueError: ZeRO inference only makes sense with ZeRO Stage 3 - please adjust your config\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 1).\u001b[0m Press Control-C to abort syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: / 0.027 MB of 0.027 MB uploaded (0.000 MB deduped)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/epoch \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/global_step \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/learning_rate \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/epoch 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/global_step 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/learning_rate 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss 0.6128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  View run \u001b[33methereal-blaze-27\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/utensil/falcon-qlora/runs/d8he4ei0\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230614_075239-d8he4ei0/logs\u001b[0m\n",
            "Exception in thread NetStatThr:\n",
            "Traceback (most recent call last):\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/threading.py\", line 980, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/threading.py\", line 917, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/wandb/sdk/wandb_run.py\", line 257, in check_network_status\n",
            "    self._loop_check_status(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/wandb/sdk/wandb_run.py\", line 213, in _loop_check_status\n",
            "    local_handle = request()\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/wandb/sdk/interface/interface.py\", line 797, in deliver_network_status\n",
            "    return self._deliver_network_status(status)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/wandb/sdk/interface/interface_shared.py\", line 601, in _deliver_network_status\n",
            "    return self._deliver_record(record)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/wandb/sdk/interface/interface_shared.py\", line 560, in _deliver_record\n",
            "    handle = mailbox._deliver_record(record, interface=self)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/wandb/sdk/lib/mailbox.py\", line 455, in _deliver_record\n",
            "    interface._publish(record)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/wandb/sdk/interface/interface_sock.py\", line 51, in _publish\n",
            "    self._sock_client.send_record_publish(record)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/wandb/sdk/lib/sock_client.py\", line 221, in send_record_publish\n",
            "    self.send_server_request(server_req)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/wandb/sdk/lib/sock_client.py\", line 155, in send_server_request\n",
            "    self._send_message(msg)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/wandb/sdk/lib/sock_client.py\", line 152, in _send_message\n",
            "    self._sendall_with_error_handle(header + data)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/wandb/sdk/lib/sock_client.py\", line 130, in _sendall_with_error_handle\n",
            "    sent = self._sock.send(data)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "Traceback (most recent call last):\n",
            "  File \"/root/miniconda3/envs/py3.9/bin/accelerate\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py\", line 45, in main\n",
            "    args.func(args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/launch.py\", line 948, in launch_command\n",
            "    simple_launcher(args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/launch.py\", line 604, in simple_launcher\n",
            "    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n",
            "subprocess.CalledProcessError: Command '['/root/miniconda3/envs/py3.9/bin/python3', 'scripts/finetune.py', 'examples/falcon/config-40b-qlora.yml', '--deepspeed', 'ds_config.json']' returned non-zero exit status 1.\n"
          ]
        }
      ],
      "source": [
        "!accelerate launch scripts/finetune.py examples/falcon/config-40b-qlora.yml --deepspeed ds_config.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIfs-AbKDY7K"
      },
      "source": [
        "## #2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6U1u8RtbDY7K",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!cat examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXasvnIEzhLu"
      },
      "outputs": [],
      "source": [
        "%%writefile ds_config.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnz7AF94LJTa"
      },
      "outputs": [],
      "source": [
        "#%%writefile examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xR8QPcw3DY7K",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!accelerate launch scripts/finetune.py examples/falcon/config-40b-qlora.yml --deepspeed ds_config.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcbjGJ2GDY7L"
      },
      "source": [
        "## #3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6hvCkF0DY7L",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!cat examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwPMxEdeLLDu"
      },
      "outputs": [],
      "source": [
        "#%%writefile examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VoEOAyfzhLv"
      },
      "source": [
        "Config from https://github.com/AetherCortex/Llama-X/blob/main/src/configs/deepspeed_config.json, many thanks to Eric!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRuxOMXtzhLv"
      },
      "outputs": [],
      "source": [
        "%%writefile ds_config.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XsSrgMvDY7L",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!accelerate launch scripts/finetune.py examples/falcon/config-40b-qlora.yml --deepspeed ds_config.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVwP73VWDY7L"
      },
      "source": [
        "## #4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFGHaYx2zhLw"
      },
      "outputs": [],
      "source": [
        "%%writefile ds_config.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3_tDMBqDY7L",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!cat examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzFuqSRNLL5_"
      },
      "outputs": [],
      "source": [
        "#%%writefile examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VIorWx3DY7M",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!accelerate launch scripts/finetune.py examples/falcon/config-40b-qlora.yml --deepspeed ds_config.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TrgHEoizhLx"
      },
      "source": [
        "## #5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vp20OUjzhLx"
      },
      "outputs": [],
      "source": [
        "%%writefile ds_config.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpJG6JK2zhLx",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!accelerate launch scripts/finetune.py examples/falcon/config-40b-qlora.yml --deepspeed ds_config.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrnqGCBezhLy"
      },
      "source": [
        "## #6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAcxGgdvzhLy"
      },
      "outputs": [],
      "source": [
        "%%writefile ds_config.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aEj2fAgzhLz"
      },
      "outputs": [],
      "source": [
        "!accelerate launch scripts/finetune.py examples/falcon/config-40b-qlora.yml --deepspeed ds_config.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UivJrMd8zhLz"
      },
      "source": [
        "## #7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99raQX83zhLz"
      },
      "outputs": [],
      "source": [
        "!cat examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubZ7fVBVzhLz"
      },
      "outputs": [],
      "source": [
        "!accelerate launch scripts/finetune.py examples/falcon/config-40b-qlora.yml --deepspeed ds_config.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UuaiP0pUzhL0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wzy_ZDY4zhL0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFWDQTL_vGSL"
      },
      "source": [
        "# Upload"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spqeHYEGDY7Q"
      },
      "source": [
        "### Upload checkpoints to HF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkekyF8DKerI"
      },
      "outputs": [],
      "source": [
        "%cd /content/axolotl-trained/falcon-qlora-40b-gsm8k/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eL8SpVYpNY1y"
      },
      "outputs": [],
      "source": [
        "!ls -lhta |grep checkpoint-"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pj_79rGoKkA_"
      },
      "outputs": [],
      "source": [
        "!ls -lhta |grep checkpoint- | awk 'NR > 1 {print $9}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XK8IWmrtKvlJ"
      },
      "outputs": [],
      "source": [
        "# ls -lhta |grep checkpoint- | awk 'NR > 1 {print $9}' | xargs rm -rf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgyQDH-YDY7R"
      },
      "outputs": [],
      "source": [
        "!python /workspace/llm-playground/helper/storage.py utensil/axolotl-trained /content/ -u"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5TnMCeLDY7M"
      },
      "source": [
        "## Below are ad hoc cells handling issues during training\n",
        "\n",
        "current out dir:\n",
        "\n",
        "```\n",
        "/content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdCBx0UZDY7M"
      },
      "source": [
        "### Force release VRAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0VdzsqSDY7M"
      },
      "outputs": [],
      "source": [
        "# First interupt the kernel, wait a few seconds then run this to kill finetune to release VRAM\n",
        "!ps aux|grep python|grep finetune|awk '{print $2}'|xargs kill"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bT7StF_PDY7N"
      },
      "source": [
        "### Clean the finetuned model and all checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FT4c-7KHDY7N"
      },
      "outputs": [],
      "source": [
        "# Only run this to start over\n",
        "!rm -rf /content/axolotl-trained/falcon-qlora-40b-gsm8k/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6s4xi9jDY7N"
      },
      "source": [
        "### Zip the prepared dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZ-aBbwEDY7N"
      },
      "outputs": [],
      "source": [
        "!apt install zip\n",
        "!zip -r last_run_prepared.zip -xi last_run_prepared"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BFC20jMDY7N"
      },
      "source": [
        "### Monitoring GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgPaXew2DY7O"
      },
      "outputs": [],
      "source": [
        "# Run this in a seperate terminal\n",
        "!nvitop -m full"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYFlPnbXDY7O"
      },
      "source": [
        "### Fix DISK FULL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yv5CWZu7DY7O"
      },
      "outputs": [],
      "source": [
        "%cd /"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvqYxcpZDY7P"
      },
      "outputs": [],
      "source": [
        "!du -d 2 -h|grep G"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIIJxrv1DY7P"
      },
      "outputs": [],
      "source": [
        "!du -d 2 -h /root/.local"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25FTebpHDY7P"
      },
      "outputs": [],
      "source": [
        "!rm -rf /root/.local/share/Trash/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8K4d-2ocDY7P"
      },
      "outputs": [],
      "source": [
        "!rm -rf /root/.local/share/wandb/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtOoZLugDY7P"
      },
      "outputs": [],
      "source": [
        "!rm -rf /root/.cache/wandb/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17SexQnqDY7P"
      },
      "source": [
        "### Check who is using GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kE8A5crNDY7P",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!apt install lsof"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwNCASirDY7Q",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!lsof /dev/nvidia*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxZv_qjgDY7Y"
      },
      "source": [
        "### A new bash without tmux etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDkJAXwTDY7Z"
      },
      "outputs": [],
      "source": [
        "!bash --norc --noprofile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pR3s54THDY7Z"
      },
      "source": [
        "### Clean up all checkpoints but last one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdO1EybEDY7Z"
      },
      "outputs": [],
      "source": [
        "cd /content/axolotl-trained/falcon-qlora-40b-gsm8k/ && ls -lhta |grep checkpoint- | awk 'NR > 1 {print $9}' | xargs rm -rf"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}