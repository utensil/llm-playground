# Daily Picks

This is for tracking daily papers, daily news, my daily discoveries/thoughts/work in the area.

Inspired by [GenAI_LLM_timeline](https://github.com/hollobit/GenAI_LLM_timeline) and [Daily Papers](https://papers.labml.ai/papers/daily/) but personalized and focused.

- Milestone-ish models/datasets/apps are categorized as 🚀News, even if they come with papers.
- 📚Papers are for better understanding the mechanisms and not just a new model trained differently, good blogs are also counted as papers.
- ⚡Discoveries are what changed my perspective or practice.
- News are dated by the time they happened. Discoveries and papers are dated by the time I noticeed their importance[^1].
- Style: only key words in the table, extra info should be available via the link or the food note.

|    Date    | 📚Papers | 🚀News                                                 | ⚡Discoveries | 🧠Thoughts/work |
| :--------: | :----- | :--------------------------------------------------- | :---------- | :------------ |
| 6.7 | [INSTRUCTEVAL](https://arxiv.org/abs/2306.04757) |  | |
| 6.6 | [InstructZero](https://arxiv.org/abs/2306.03082) |  | |
| 6.5 | [Video-LLaMA](https://arxiv.org/abs/2306.02858) |  | |
| 6.5 | [RLHF-APA](https://arxiv.org/abs/2306.02231) |  | |
| 6.5 | [Orca](https://arxiv.org/abs/2306.02707) |  | |
| 6.5 | [Tr+SD](https://arxiv.org/abs/2306.02531) |  | |
| 6.2 | [RefinedWeb](https://arxiv.org/abs/2306.01116) |  | |
| 6.2 | [StyleDrop](https://arxiv.org/abs/2306.00983) |  | |
| 6.1 | [Hiera: A Hierarchical ViT](https://arxiv.org/abs/2306.00989) |  | |
| 6.1 | [Hidden Language in SD](https://arxiv.org/abs/2306.00966) |  | |
| 6.1 | [Birth of a Transformer](https://arxiv.org/abs/2306.00802) |  | |
| 6.1 | [ReviewerGPT](https://arxiv.org/abs/2306.02707) |  | |
| 5.31 | [Grammar Prompting for DSL](https://arxiv.org/abs/2305.19234) |  | |
| 5.28 | [Geometric Algebra Transformers](https://arxiv.org/abs/2305.18415) |  | |
| 5.26 | | [Falcon 7B/40B & RefinedWeb](https://huggingface.co/tiiuae/falcon-40b-instruct) |  |
| 5.26 | | [Gorilla](https://gorilla.cs.berkeley.edu/) | [TF Agents](https://github.com/huggingface/transformers/pull/23214) |
| 5.24 | [Recursively](https://arxiv.org/abs/2305.14699) | |
| 5.23 | [VanillaNet](https://arxiv.org/abs/2305.14342) | |
| 5.23 | [Sophia](https://arxiv.org/abs/2305.14342) | |
| 5.23 | [QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes) | [guanaco-65B](https://huggingface.co/TheBloke/guanaco-65B-HF) |
| 5.22 | [RWKV](https://arxiv.org/abs/2305.13048) |
| 5.22 | | [GPT4All 13B Snoozy](https://twitter.com/andriy_mulyar/status/1654544917035720705) |
| 5.21 | | | [The Little Book](https://fleuret.org/public/lbdl.pdf) |
| 5.20 | [Thought Forest](https://arxiv.org/abs/2302.06706) |
| 5.20 | |  [248 H100 SXM5s](https://twitter.com/boborado/status/1659608452849897472?s=20) | [Cooperation](https://arxiv.org/abs/2305.10449) & [Hyena](https://arxiv.org/abs/2302.10866) |
| 5.20 | [CodeCompose](https://arxiv.org/abs/2305.12050) |
| 5.18 | [Meaning](https://arxiv.org/abs/2305.11169) |
| 5.18 | [LIMA](https://arxiv.org/abs/2305.11206) |
| 5.18 | [Embodied Experiences](https://arxiv.org/abs/2305.10626) |
| 5.17 | [DoReMi](https://arxiv.org/abs/2305.10429) |
| 5.17 | [Safe-RLHF](https://github.com/PKU-Alignment/safe-rlhf) |
| 5.17 | [ToT](https://arxiv.org/abs/2305.10601) |
| 5.16 | [StructGPT](https://arxiv.org/abs/2305.09645) |
| 5.15 | | | [{{Guidance}}](https://github.com/microsoft/guidance) |
| 5.13 | |  [Prompt Leak](https://twitter.com/marvinvonhagen/status/1657060506371346432) |
| 5.13 | |  [CodeT5+](https://github.com/salesforce/CodeT5/tree/main/CodeT5%2B) |
| 5.12 | | | [spacy-llm](https://github.com/explosion/spacy-llm) |
| 5.12 | [TinyStories](https://arxiv.org/abs/2305.07759) |
| 5.12 | [MEGABYTE](https://arxiv.org/abs/2305.07185) |
| 5.10 | |  [IMAGEBIND](https://ai.facebook.com/blog/imagebind-six-modalities-binding-ai/) |
| 5.10 | | | [Named Tensor Notation](https://namedtensor.github.io/) |
| 5.6 | | [MMS](https://github.com/facebookresearch/fairseq/tree/main/examples/mms) |
| 5.6 | | | [MEMIT](https://arxiv.org/abs/2210.07229) & [REMEDI](https://arxiv.org/abs/2304.00740) |
| 5.5 | | [RedPajama-INCITE 7B](https://www.together.xyz/blog/redpajama-models-v1) |
| 5.5 | | [OpenAlpaca](https://github.com/yxuansu/OpenAlpaca) |
| 5.5 | [ALiBi](https://arxiv.org/abs/2108.12409) & [Lion](https://arxiv.org/abs/2302.06675) | [MPT-7B](https://www.mosaicml.com/blog/mpt-7b) | [Composer](https://github.com/mosaicml/composer) & [StreamingDataset](https://github.com/mosaicml/streaming) && [LLM Foundry](https://github.com/mosaicml/llm-foundry) |
| 5.5 | [SELF-ALIGN](https://arxiv.org/abs/2305.03047) | [IBM Dromedary 65B](https://mitibmdemos.draco.res.ibm.com/dromedary) |
| 5.4 | [APO](https://arxiv.org/abs/2305.03495) |
| 5.4 | [Multi Query Attention](https://arxiv.org/abs/1911.02150) & [Fill-in-the-Middle objective](https://arxiv.org/abs/2207.14255) | [StarCoder-15B](https://huggingface.co/blog/starcoder) | [bigcode/Megatron-LM](https://github.com/bigcode-project/Megatron-LM) |
| 5.3 | [Sourcegraph Cody](https://about.sourcegraph.com/cody) | 
| 5.3 | [FasterTransformer](https://github.com/NVIDIA/FasterTransformer) | [replit-code-v1-3b](https://huggingface.co/replit/replit-code-v1-3b) |
| 5.3 | | [OpenLLaMA 7B](https://lmsys.org/blog/2023-05-03-arena/) |
| 5.3 | | [Chatbot Arena](https://github.com/openlm-research/open_llama) |
| 5.3 | [Distilling Step-by-Step](https://arxiv.org/abs/2305.02301) |
| 5.2 | [Unlimiformer](https://arxiv.org/abs/2305.01625) |
| 5.2 | [Loss Landscapes](https://openreview.net/forum?id=QC10RmRbZy9) |
| 5.1 | [Self-Notes](https://arxiv.org/abs/2305.00833) |
| 4.29 | | [Lamini 12B](https://lamini.ai/blog/introducing-lamini) |
| 4.28 | | [StableVicuna 13B](https://stability.ai/blog/stablevicuna-open-source-rlhf-chatbot)[^6] |
| 4.28 | [Causal Reasoning & LLM](https://arxiv.org/abs/2305.00050) |
| 4.28 | [Iterative Bootstrapping](https://arxiv.org/abs/2304.11657) |
| 4.27 | [Formal Transformers](https://arxiv.org/abs/2207.09238) |
| 4.26 | [Transformers](https://arxiv.org/abs/2304.10557) |
| 4.26 | | | [HELM](https://github.com/stanford-crfm/helm) & [benchmarks](https://twitter.com/abacaj/status/1648881680835387392)
| 4.26 | | | [Silent Bugs](https://ppwwyyxx.com/blog/2020/Fight-Against-Silent-Bugs-in-Deep-Learning-Libraries/) |
| 4.26 | | | [Kernl](https://www.kernl.ai/) |
| 4.21 | | | [137 emergent abilities](https://www.jasonwei.net/blog/emergence) |
| 4.21 | | | Training [logbook](https://github.com/facebookresearch/metaseq/tree/main/projects/OPT/chronicles) & [metric](https://wandb.ai/eleutherai/neox) |
| 4.21 | | | [axolotl](https://github.com/winglian/axolotl) & [genv](https://github.com/run-ai/genv) |
| 4.20 |  [Verifiability](https://arxiv.org/abs/2304.09848) |
| 4.19 | | |  [GPTCache](https://github.com/zilliztech/GPTCache) |
| 4.19 | [FlashAttention](https://arxiv.org/abs/2205.14135) |  [StableLM](https://stability.ai/blog/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models)  | [GPT-NeoX](https://github.com/EleutherAI/gpt-neox) & [Megatron](https://github.com/NVIDIA/Megatron-LM) |
| 4.19 | | |  [meerkat](https://github.com/HazyResearch/meerkat)[^5] |
| 4.19 | | | [CAMEL](https://github.com/lightaime/camel) & [chatarena](https://github.com/chatarena/chatarena) |
| 4.18 | [FT v.s. LoRA](https://arxiv.org/abs/2304.08109) | [BELLE](https://github.com/LianjiaTech/BELLE) |
| 4.18 | | [LLaVA](https://github.com/haotian-liu/LLaVA) |
| 4.17 | | |   [Alpaca-CoT](https://github.com/PhoebusSi/Alpaca-CoT) |
| 4.17 | |  [RedPajama-Data](https://github.com/togethercomputer/RedPajama-Data) |
| 4.17 | |  | [alpaca_lora_4bit](https://github.com/johnsmith0031/alpaca_lora_4bit) |
| 4.17 | |  | [Transformer Family](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/) |
| 4.16 |  [LLMs + Symbolic Solvers](https://arxiv.org/abs/2304.09102) |
| 4.16 |   [`suggest_premises`](https://github.com/BartoszPiotrowski/lean-premise-selection) |
| 4.15 | |[MiniGPT-4](https://minigpt-4.github.io/) |
| 4.15 | |[web-llm](https://github.com/mlc-ai/web-llm) |
| 4.14 | | | [Buzzard's talk](https://leanprover.zulipchat.com/#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Bangalore.20talk.20on.20LLM.2BITP/near/348388948) |
| 4.14 | | | [ProofNet](https://github.com/zhangir-azerbayev/ProofNet) |
| 4.14 | [Multimodal C4](https://arxiv.org/abs/2304.06939) |
| 4.13 | [CodeWhisperer](https://aws.amazon.com/cn/blogs/aws/amazon-codewhisperer-free-for-individual-use-is-now-generally-available/) |
| 4.13 | [GPT-4 Annotating](https://arxiv.org/abs/2304.06588) |
| 4.12 | | | [LLMPruner](https://github.com/yangjianxin1/LLMPruner) |
| 4.12 | [Galactic ChitChat](https://arxiv.org/abs/2304.05406) |
| 4.12 | |  [Dolly v2](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm) |
| 4.12 | |  [DeepSpeed Chat](https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-chat) |
| 4.11 | [Toxicity](https://arxiv.org/abs/2304.05335) |
| 4.11 | [Privacy Attacks](https://arxiv.org/abs/2304.05197) |
| 4.11 | [Self-Debug](https://arxiv.org/abs/2304.05128) |
| 4.11 | [Auto-Sci](https://arxiv.org/abs/2304.05332) |
| 4.12 | | | [RunPod.io](https://runpod.io?ref=km0th85l) |
| 4.10 |  [pal](https://github.com/reasoning-machines/pal) |
| 4.9 | | |  [Patrick's talk](https://www.youtube.com/watch?v=tp_h3vzkObo) |
| 4.9 |  [ACT]([Trusted-AI/adversarial-robustness-toolbox](https://github.com/Trusted-AI/adversarial-robustness-toolbox)) |
| 4.9 | | |  [dagster](https://github.com/dagster-io/dagster) & [mage-ai](https://github.com/mage-ai/mage-ai) |
| 4.8 | | |  [data-centric-AI](https://github.com/daochenzha/data-centric-AI) |
| 4.8 | [Training Recipe](https://wandb.ai/craiyon/report/reports/Recipe-Training-Large-Models--VmlldzozNjc4MzQz) | 
| 4.7 | |  [lightning](https://github.com/Lightning-AI/lightning) & [lit-llama](https://github.com/Lightning-AI/lit-llama) |
| 4.7 |  | [Vicuna](https://github.com/lm-sys/FastChat) |
| 4.5 | | [SAM](https://ai.facebook.com/research/publications/segment-anything/) |
| 4.5 | | [StackLLaMA & trl](https://huggingface.co/blog/stackllama) |
| 4.4 | | |  [text-generation-webui](https://github.com/oobabooga/text-generation-webui) |
| 4.4 | [LLM-Adapters](https://arxiv.org/abs/2304.01933) |
| 4.3 | | | [ChatML](https://github.com/openai/openai-python/blob/main/chatml.md) |
| 4.3 | | [Koala](https://bair.berkeley.edu/blog/2023/04/03/koala/) |
| 4.2 |  [Code Self-Improvement](https://arxiv.org/abs/2304.01228) |
| 4.2 | | | [ChuanhuChatGPT](https://github.com/GaiZhenbiao/ChuanhuChatGPT) |
| 4.1 |  |  [LMFlow](https://github.com/OptimalScale/LMFlow) |
| 3.31 | [Choose Your Weapon](https://arxiv.org/abs/2304.06035) |
| 3.30 | [Humans in Humans Out](https://arxiv.org/abs/2303.17276) |
| 3.30 | | [galpaca-30b](https://huggingface.co/GeorgiaTechResearchInstitute/galpaca-30b) |
| 3.30 | | [BloombergGPT](https://arxiv.org/abs/2303.17564) |
| 3.30 | | [Auto-GPT](https://github.com/Torantulino/Auto-GPT) |
| 3.29 | | | [guardrails](https://github.com/ShreyaR/guardrails) & [lmql](https://github.com/eth-sri/lmql) & [kor](https://github.com/eyurtsev/kor) |
| 3.29 | | [GPT4All](https://github.com/nomic-ai/gpt4all) |
| 3.29 | | [LLaMA-Adapter](https://arxiv.org/abs/2303.16199) |
| 3.29 | | | [llama_index](https://github.com/jerryjliu/llama_index) | 
| 3.28 | | [OpenFlamingo](https://laion.ai/blog/open-flamingo/) |
| 3.28 | | [Cerebras-GPT](https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/) |
| 3.27 | | [LeCun's talk](https://twitter.com/ylecun/status/1640133789199347713) |
| 3.26 | [Low-Rank Simplicity Bias](https://openreview.net/forum?id=dn4B7Mes2z) |
| 3.25 | [APE](https://arxiv.org/abs/2211.01910) |
| 3.24 | | [Dolly](https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html) |
| 3.23 | | | [dalai](https://github.com/cocktailpeanut/dalai)[^4] |
| 3.23 | | [ChatGPT Plugins](https://openai.com/blog/chatgpt-plugins) |
| 3.23 | | | [Cursor.so](http://cursor.so/)[^3] |
| 3.22	| | [Sparks of AGI](https://arxiv.org/abs/2303.12712) |
| 3.20 | | [ChatGPT outage](https://openai.com/blog/march-20-chatgpt-outage) | 
| 3.16 | | [Alpaca LoRA](https://twitter.com/_akhaliq/status/1636416647518097408) |
| 3.15	| | [GPT-4 TR](https://arxiv.org/abs/2303.08774) |
|    3.14    |        | [GPT-4](https://openai.com/research/gpt-4)           |
| 3.13 | | [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html) |
|    3.2     |        | | [miniF2F](https://github.com/openai/miniF2F)       |
|    3.1     |        | [ChatGPT API](https://openai.com/blog/chatgpt)       |
|    3.1     |        | | [galai](https://github.com/paperswithcode/galai) |
| 2.26 | | | [ColossalAI](https://github.com/hpcaitech/ColossalAI)[^2]
| 2.24 | | [LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai)
|    2.10    |        | [ChatGPT Plus](https://openai.com/blog/chatgpt-plus) |
|    2.7    |        | [New Bing](https://blogs.microsoft.com/blog/2023/02/07/reinventing-search-with-a-new-ai-powered-microsoft-bing-and-edge-your-copilot-for-the-web/) |
|    2023    |
| 2022.11.30 |        | [ChatGPT](https://openai.com/blog/chatgpt)           |
| 2021.08.10 |        | [Codex](https://openai.com/blog/openai-codex)        |
| 2020.05.28 |        | [GPT-3](https://arxiv.org/abs/2005.14165)            |


## TODO

Decide whether include them and determine dates:

|    Date    | Papers | News                                                 | Discoveries | Thoughts/work |
| :--------: | :----- | :--------------------------------------------------- | :---------- | :------------ |
| 4.18 | [SPQA](https://danielmiessler.com/blog/spqa-ai-architecture-replace-existing-software/) |
| 4.9 | | |  [spaCy](https://github.com/explosion/spaCy) |
| 3.31 | | |  [simple-llm-finetuner](https://github.com/lxe/simple-llm-finetuner) |
| 3.19 | | [Web AI](https://github.com/visheratin/web-ai) | 
| 1.6 |  |  [NeevaAI](https://neeva.com/blog/introducing-neevaai) |

## Related curated lists

### Papers & Notes

- [Mooler0410/LLMsPracticalGuide](https://github.com/Mooler0410/LLMsPracticalGuide) - A curated list of practical guide resources of LLMs. 
- [thunlp/PromptPapers](https://github.com/thunlp/PromptPapers) - Must-read papers on prompt-based tuning for pre-trained language models.
- [foocker/deeplearningtheory](https://github.com/foocker/deeplearningtheory) 
- [dair-ai/ML-Course-Notes](https://github.com/dair-ai/ML-Course-Notes) - 🎓 Sharing machine learning course / lecture notes.
- [ml4code](https://ml4code.github.io/papers.html) - A Survey of Machine Learning for Big Code and Naturalness
- [Everything-LLMs-And-Robotics](https://github.com/jrin771/Everything-LLMs-And-Robotics) - The world's largest GitHub Repository for LLMs + Robotics

### Models

- [Longyichen/Alpaca-family-library](https://github.com/Longyichen/Alpaca-family-library) - Summarize all low-cost replication methods for Chatgpt. 
- [imaurer/awesome-decentralized-llm](https://github.com/imaurer/awesome-decentralized-llm) - Collection of LLM resources that can be used to build products you can "own" or to perform reproducible research.
- [nichtdax/awesome-totally-open-chatgpt](https://github.com/nichtdax/awesome-totally-open-chatgpt) - A list of totally open alternatives to ChatGPT
- [FreedomIntelligence/LLMZoo](https://github.com/FreedomIntelligence/LLMZoo) - ⚡LLM Zoo is a project that provides data, models, and evaluation benchmark for large language models.⚡
- [arjunbansal/awesome-oss-llm-ift-rlhf](https://github.com/arjunbansal/awesome-oss-llm-ift-rlhf) - Collection of open source implementations of LLMs with IFT and RLHF that are striving to get to ChatGPT level of performance
- [stanford-crfm/ecosystem-graphs](https://github.com/stanford-crfm/ecosystem-graphs) - an ongoing effort to track the foundation model ecosystem

### Training

- [zhilizju/Awesome-instruction-tuning](https://github.com/zhilizju/Awesome-instruction-tuning) - A curated list of awesome instruction tuning datasets, models, papers and repositories.
- [yaodongC/awesome-instruction-dataset](https://github.com/yaodongC/awesome-instruction-dataset) - A collection of open-source dataset to train instruction-following LLMs (ChatGPT,LLaMA,Alpaca)
- [PhoebusSi/Alpaca-CoT](https://github.com/PhoebusSi/Alpaca-CoT) - We unified the interfaces of instruction-tuning data (e.g., CoT data), multiple LLMs and parameter-efficient methods (e.g., lora, p-tuning) together for easy use
- [visenger/awesome-mlops](https://github.com/visenger/awesome-mlops) - A curated list of references for MLOps

### Reasoning

- [lupantech/dl4math](https://github.com/lupantech/dl4math) - Resources of deep learning for mathematical reasoning (DL4MATH).
- [tensorush/Awesome-Maths-Learning](https://github.com/tensorush/Awesome-Maths-Learning) - :sunglasses: :scroll: Collection of the most awesome Maths learning resources in the form of notes, videos and cheatsheets.

### Prompting

- [dair-ai/Prompt-Engineering-Guide](https://github.com/dair-ai/Prompt-Engineering-Guide) - 🐙 Guides, papers, lecture, notebooks and resources for prompt engineering

### Apps

- [reorx/awesome-chatgpt-api](https://github.com/reorx/awesome-chatgpt-api) - Curated list of apps and tools that not only use the new ChatGPT API, but also allow users to configure their own API keys, enabling free and on-demand usage of their own quota.

[^1]: Models and datasets are already tracked seperately as simple machine-digestable files as `models.txt` and `datasets.txt`, and some on [my likes](https://huggingface.co/utensil/activity/likes). Repos are tracked by [my stars](https://github.com/utensil/awesome-stars/blob/master/topics.md), mostly in topic `chatgpt`, `chatgpt-api`, `ai`, `artificial-intelligence`, `data-science` and `data-analysis`, also in my star list [lean-llm](https://github.com/stars/utensil/lists/lean-llm) focusing on the building blocks of applying LLMs to the ITP/ATP area.

[^2]: The first open source RLHF pipeline

[^3]: Helped me experience prompt-based coding infinitely

[^4]: Helped me testing LLaMA and Alpaca locally

[^5]: Meerkat is a Python library for interactively exploring unstructured data with foundation models that understand them, you can also seamlessly switch between augmented data frames and reactive GUIs for easy verification and feedback.

[^6]: The AI World’s First Open Source RLHF LLM Chatbot
